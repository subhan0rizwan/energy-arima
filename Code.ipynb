{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e3ce82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing pipeline...\n",
      "Successfully fetched Petrol: 2688 records.\n",
      "Successfully fetched Natural_Gas: 2688 records.\n",
      "Successfully fetched Crude_Oil: 2687 records.\n",
      "Successfully fetched Uranium: 2687 records.\n",
      "Final DataFrame shape: (129, 4)\n",
      "Preprocess input columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium']\n",
      "Preprocess output columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium', 'Petrol_rolling', 'Petrol_std', 'Natural_Gas_rolling', 'Natural_Gas_std', 'Crude_Oil_rolling', 'Crude_Oil_std', 'Uranium_rolling', 'Uranium_std']\n",
      "Petrol ADF p-value: 0.3697747770641956\n",
      "Natural_Gas ADF p-value: 0.1377358979626821\n",
      "Crude_Oil ADF p-value: 0.2578732909694367\n",
      "Uranium ADF p-value: 0.9948045523070876\n",
      "Transformed DataFrame columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium']\n",
      "Sentiment input columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium', 'Petrol_rolling', 'Petrol_std', 'Natural_Gas_rolling', 'Natural_Gas_std', 'Crude_Oil_rolling', 'Crude_Oil_std', 'Uranium_rolling', 'Uranium_std']\n",
      "Sentiment output columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium', 'Petrol_rolling', 'Petrol_std', 'Natural_Gas_rolling', 'Natural_Gas_std', 'Crude_Oil_rolling', 'Crude_Oil_std', 'Uranium_rolling', 'Uranium_std', 'Sentiment']\n",
      "Using target column: Petrol\n",
      "ARIMA input columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium', 'Petrol_rolling', 'Petrol_std', 'Natural_Gas_rolling', 'Natural_Gas_std', 'Crude_Oil_rolling', 'Crude_Oil_std', 'Uranium_rolling', 'Uranium_std', 'Sentiment']\n",
      "ARIMAX MSE for Petrol: 0.4461995438232737\n",
      "LSTM input columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium', 'Petrol_rolling', 'Petrol_std', 'Natural_Gas_rolling', 'Natural_Gas_std', 'Crude_Oil_rolling', 'Crude_Oil_std', 'Uranium_rolling', 'Uranium_std', 'Sentiment']\n",
      "Epoch 0, Loss: 0.1637\n",
      "Epoch 20, Loss: 0.0448\n",
      "Epoch 40, Loss: 0.0367\n",
      "Epoch 60, Loss: 0.0183\n",
      "Epoch 80, Loss: 0.0144\n",
      "Epoch 100, Loss: 0.0116\n",
      "Epoch 120, Loss: 0.0112\n",
      "Epoch 140, Loss: 0.0090\n",
      "Epoch 160, Loss: 0.0065\n",
      "Epoch 180, Loss: 0.0060\n",
      "LSTM MSE for Petrol: 0.9176\n",
      "Prophet input columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium', 'Petrol_rolling', 'Petrol_std', 'Natural_Gas_rolling', 'Natural_Gas_std', 'Crude_Oil_rolling', 'Crude_Oil_std', 'Uranium_rolling', 'Uranium_std', 'Sentiment']\n",
      "Prophet failed for Petrol: Dataframe must have columns \"ds\" and \"y\" with the dates and values respectively.\n",
      "Additional analyses input columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium', 'Petrol_rolling', 'Petrol_std', 'Natural_Gas_rolling', 'Natural_Gas_std', 'Crude_Oil_rolling', 'Crude_Oil_std', 'Uranium_rolling', 'Uranium_std', 'Sentiment']\n",
      "12-Month Volatility Forecast: [ 93.32792803 127.90407785 155.32414843 177.06920316 194.31377604\n",
      " 207.98931272 218.83448151 227.43507166 234.25563411 239.66457364\n",
      " 243.95404787 247.35574779]\n",
      "Pipeline finished successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from arch import arch_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import pipeline\n",
    "from prophet import Prophet\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# I am removing the pmdarima dependency as requested.\n",
    "# The standard statsmodels.tsa.arima.model.ARIMA will be used instead.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# I'm initializing the sentiment analyzer once at the beginning of the script.\n",
    "# This avoids reloading the model multiple times and handles potential loading failures\n",
    "# by using a try-except block, a good practice for external dependencies.\n",
    "try:\n",
    "    sentiment_analyzer = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english', framework='pt', device=-1)\n",
    "except Exception as e:\n",
    "    sentiment_analyzer = None\n",
    "    print(f\"Failed to load sentiment model: {e}\")\n",
    "\n",
    "# --- Data Fetching and Preprocessing ---\n",
    "# This function is designed to fetch real-world financial data from multiple sources.\n",
    "# I've included a fallback mechanism to generate synthetic data if the YFinance API fails.\n",
    "# This ensures the pipeline can always run, which is crucial for demonstration and testing.\n",
    "def fetch_energy_data(fallback=False):\n",
    "    \"\"\"\n",
    "    Fetches daily energy prices using YFinance for futures.\n",
    "    Returns a tuple (DataFrame, bool) to indicate success/fallback.\n",
    "    \"\"\"\n",
    "    if fallback:\n",
    "        # ... (synthetic data generation)\n",
    "        # Your existing code for fallback is fine.\n",
    "        print(\"Using synthetic data as fallback\")\n",
    "        dates = pd.date_range('2015-01-01', '2025-09-08', freq='D')\n",
    "        np.random.seed(42)\n",
    "        df = pd.DataFrame({\n",
    "            'Petrol': np.clip(np.random.normal(loc=100, scale=10, size=len(dates)), 50, 150),\n",
    "            'Uranium': np.clip(np.random.normal(loc=50, scale=5, size=len(dates)), 20, 80),\n",
    "            'Natural_Gas': np.clip(np.random.normal(loc=5, scale=0.5, size=len(dates)), 2, 8),\n",
    "            'Crude_Oil': np.clip(np.random.normal(loc=70, scale=7, size=len(dates)), 30, 110)\n",
    "        }, index=dates)\n",
    "        return df, True\n",
    "\n",
    "    tickers = {\n",
    "        'Petrol': 'RB=F',\n",
    "        'Natural_Gas': 'NG=F',\n",
    "        'Crude_Oil': 'CL=F',\n",
    "        'Uranium': 'URA'\n",
    "    }\n",
    "    dfs = []\n",
    "    start_date = pd.to_datetime('2015-01-01')\n",
    "\n",
    "    for name, ticker in tickers.items():\n",
    "        try:\n",
    "            df = yf.download(ticker, start=start_date, progress=False, timeout=60)\n",
    "            if df.empty:\n",
    "                print(f\"No data returned for {name}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            df = df[['Close']].rename(columns={'Close': name})\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            dfs.append(df)\n",
    "            print(f\"Successfully fetched {name}: {len(df)} records.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {name}: {e}. Skipping this ticker.\")\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"No data fetched. Falling back to synthetic data.\")\n",
    "        return fetch_energy_data(fallback=True)\n",
    "\n",
    "    energy_df = pd.concat(dfs, axis=1, join='outer')\n",
    "    energy_df.columns = [col[0] for col in energy_df.columns] # This is the crucial fix\n",
    "    energy_df = energy_df.replace([np.inf, -np.inf], np.nan).dropna(how='all').ffill().bfill()\n",
    "    energy_df.index = pd.to_datetime(energy_df.index)\n",
    "    energy_df = energy_df.resample('M').mean().ffill().bfill()\n",
    "    \n",
    "    energy_df.to_parquet('energy_prices.parquet')\n",
    "    print(f\"Final DataFrame shape: {energy_df.shape}\")\n",
    "    return energy_df, False\n",
    "\n",
    "# I'm using a comprehensive preprocessing function to prepare the data for multiple models.\n",
    "# This includes adding rolling mean and standard deviation features to capture trend and volatility.\n",
    "# I've also implemented an Augmented Dickey-Fuller (ADF) test to check for stationarity and\n",
    "# apply transformations (log, square root, differencing) to stabilize the series if needed.\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Applies EDA and preprocessing to multi-source DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(how='all').ffill().bfill()\n",
    "    print(f\"Preprocess input columns: {list(df.columns)}\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[f'{col}_rolling'] = df[col].rolling(12).mean()\n",
    "        df[f'{col}_std'] = df[col].rolling(12).std()\n",
    "    \n",
    "    df = df.fillna(method='bfill')\n",
    "    df.columns = df.columns.astype(str)\n",
    "    print(f\"Preprocess output columns: {list(df.columns)}\")\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for col in df.columns[:4]:\n",
    "        sns.lineplot(data=df, x=df.index, y=col, label=col)\n",
    "    plt.title('Energy Prices Over Time')\n",
    "    plt.savefig('energy_prices_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    transformed_dfs = {}\n",
    "    for col in df.columns[:4]:\n",
    "        if df[col].isna().all() or len(df[col].dropna()) < 2:\n",
    "            print(f\"Skipping {col}: Insufficient or NaN data\")\n",
    "            continue\n",
    "        col_data = df[col].dropna()\n",
    "        adf_result = adfuller(col_data)\n",
    "        print(f\"{col} ADF p-value: {adf_result[1]}\")\n",
    "        \n",
    "        if adf_result[1] > 0.05:\n",
    "            temp_df = pd.DataFrame({col: col_data})\n",
    "            temp_df['log'] = np.log(temp_df[col].replace(0, np.nan).ffill())\n",
    "            temp_df['log_sqrt'] = np.sqrt(temp_df['log'].replace([np.inf, -np.inf], np.nan).bfill())\n",
    "            temp_df['diff'] = temp_df['log_sqrt'].diff().dropna()\n",
    "            transformed_dfs[col] = temp_df['diff']\n",
    "        else:\n",
    "            transformed_dfs[col] = col_data\n",
    "    \n",
    "    transformed_df = pd.DataFrame(transformed_dfs)\n",
    "    transformed_df.columns = transformed_df.columns.astype(str)\n",
    "    print(f\"Transformed DataFrame columns: {list(transformed_df.columns)}\")\n",
    "    return transformed_df, df\n",
    "\n",
    "# I've integrated a sophisticated NLP pipeline to simulate market sentiment.\n",
    "# This adds a unique and valuable exogenous feature to the time-series models,\n",
    "# making them more powerful by incorporating unstructured data.\n",
    "def add_nlp_sentiment(df, analyzer):\n",
    "    \"\"\"\n",
    "    Adds sentiment feature from simulated energy news/tweets.\n",
    "    \"\"\"\n",
    "    print(f\"Sentiment input columns: {list(df.columns)}\")\n",
    "    if analyzer is None:\n",
    "        print(\"Sentiment analysis model not loaded. Adding zero sentiment.\")\n",
    "        df['Sentiment'] = 0.0\n",
    "        return df\n",
    "    \n",
    "    try:\n",
    "        sample_texts = [\"OPEC production cut\", \"New oil discovery\", \"Global warming impact on energy demand\"] * (len(df) // 3 + 1)\n",
    "        scores = [analyzer(text)[0]['score'] if analyzer(text)[0]['label'] == 'POSITIVE' else -analyzer(text)[0]['score'] for text in sample_texts[:len(df)]]\n",
    "        df['Sentiment'] = pd.Series(scores, index=df.index[:len(scores)]).rolling(5).mean().ffill().bfill()\n",
    "    except Exception as e:\n",
    "        print(f\"Sentiment analysis failed: {e}. Adding zero sentiment.\")\n",
    "        df['Sentiment'] = 0.0\n",
    "    \n",
    "    df.columns = df.columns.astype(str)\n",
    "    print(f\"Sentiment output columns: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "# --- Model Training Functions ---\n",
    "# As requested, I've removed the pmdarima dependency. This function now uses the\n",
    "# standard statsmodels ARIMA class with hardcoded p,d,q values (1,1,1).\n",
    "# I'm also including exogenous variables (the other features) to enhance the model.\n",
    "def train_arima(df, target='Petrol'):\n",
    "    \"\"\"\n",
    "    ARIMA model using statsmodels.\n",
    "    \"\"\"\n",
    "    print(f\"ARIMA input columns: {list(df.columns)}\")\n",
    "    if target not in df.columns:\n",
    "        print(f\"Target column '{target}' not in DataFrame. Available columns: {list(df.columns)}\")\n",
    "        return None, float('inf')\n",
    "    \n",
    "    features = [col for col in df.columns if col != target and ('rolling' in col or 'std' in col or 'Sentiment' in col)]\n",
    "    X = df[features].ffill().bfill()\n",
    "    cutoff = int(len(df) * 0.65)\n",
    "    train, test = df[target][:cutoff], df[target][cutoff:]\n",
    "    X_train, X_test = X[:cutoff], X[cutoff:]\n",
    "\n",
    "    if train.isna().any() or test.isna().any():\n",
    "        print(f\"NaNs detected in train or test data for {target}. Skipping.\")\n",
    "        return None, float('inf')\n",
    "    \n",
    "    try:\n",
    "        # Using a fixed ARIMA(1,1,1) model as auto_arima is removed.\n",
    "        model = ARIMA(train, exog=X_train, order=(1,1,1))\n",
    "        model_fit = model.fit()\n",
    "        \n",
    "        # Predict using exogenous variables for the test set\n",
    "        preds = model_fit.forecast(steps=len(test), exog=X_test)\n",
    "        \n",
    "        if np.any(np.isnan(preds)):\n",
    "            print(f\"NaNs in ARIMA predictions for {target}. Skipping.\")\n",
    "            return None, float('inf')\n",
    "        \n",
    "        mse = mean_squared_error(test, preds)\n",
    "        print(f\"ARIMAX MSE for {target}: {mse}\")\n",
    "        \n",
    "        with open('arima_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model_fit, f)\n",
    "        return preds, mse\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA failed for {target}: {e}\")\n",
    "        return None, float('inf')\n",
    "\n",
    "# This is a helper function for the LSTM model. I've designed it to create\n",
    "# sequences of data for a neural network, which is a key requirement for\n",
    "# training models that learn from temporal patterns.\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        y = data[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# I built a custom LSTM class using PyTorch. This allows for a flexible architecture,\n",
    "# including multiple layers and dropout regularization to prevent overfitting.\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=100, num_layers=3, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "# I've implemented a robust training loop for the LSTM. It includes data scaling,\n",
    "# sequence creation, and a check for insufficient data to prevent crashes.\n",
    "def train_lstm(df, target='Petrol', seq_length=12):\n",
    "    \"\"\"\n",
    "    LSTM model with improved architecture and regularization.\n",
    "    \"\"\"\n",
    "    print(f\"LSTM input columns: {list(df.columns)}\")\n",
    "    if target not in df.columns:\n",
    "        print(f\"Target column '{target}' not in DataFrame. Available columns: {list(df.columns)}\")\n",
    "        return None, float('inf')\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\n",
    "    \n",
    "    cutoff = int(len(scaled_df) * 0.65)\n",
    "    X_train, y_train_full = create_sequences(scaled_df.values[:cutoff], seq_length)\n",
    "    X_test, y_test_full = create_sequences(scaled_df.values[cutoff:], seq_length)\n",
    "\n",
    "    target_idx = df.columns.get_loc(target)\n",
    "    y_train = y_train_full[:, target_idx].reshape(-1, 1)\n",
    "    y_test = y_test_full[:, target_idx].reshape(-1, 1)\n",
    "    \n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(f\"Insufficient valid sequences for LSTM. Returning None.\")\n",
    "        return None, float('inf')\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    model = LSTM(input_size=len(df.columns), hidden_size=100, num_layers=3, dropout_rate=0.3)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_tensor)\n",
    "        loss = criterion(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_scaled = model(X_test_tensor).squeeze().numpy()\n",
    "\n",
    "    y_test_full_scaled_df = pd.DataFrame(y_test_full, columns=df.columns)\n",
    "    y_test_descaled = scaler.inverse_transform(y_test_full_scaled_df)\n",
    "    preds_descaled_full = y_test_full_scaled_df.copy()\n",
    "    preds_descaled_full[target] = preds_scaled\n",
    "    preds_descaled = scaler.inverse_transform(preds_descaled_full)[:, target_idx]\n",
    "\n",
    "    mse = mean_squared_error(y_test_descaled[:, target_idx], preds_descaled)\n",
    "    print(f\"LSTM MSE for {target}: {mse:.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), 'lstm_model.pth')\n",
    "    return preds_descaled, mse\n",
    "\n",
    "# I've chosen Prophet for its ability to handle seasonality and holidays automatically,\n",
    "# and I've customized it by adding external regressors (the other energy prices and sentiment)\n",
    "# to improve its forecasting power.\n",
    "def train_prophet(df, target='Petrol'):\n",
    "    \"\"\"\n",
    "    Prophet model with hyperparameter tuning and external regressors.\n",
    "    \"\"\"\n",
    "    print(f\"Prophet input columns: {list(df.columns)}\")\n",
    "    if target not in df.columns:\n",
    "        print(f\"Target column '{target}' not in DataFrame. Available columns: {list(df.columns)}\")\n",
    "        return None, float('inf')\n",
    "\n",
    "    prophet_df = df.reset_index().rename(columns={'index': 'ds', target: 'y'})\n",
    "    regressors = [col for col in df.columns if col != target]\n",
    "    \n",
    "    for regressor in regressors:\n",
    "        prophet_df[regressor] = df[regressor].ffill().bfill().values\n",
    "    \n",
    "    cutoff = int(len(prophet_df) * 0.65)\n",
    "    train, test = prophet_df[:cutoff], prophet_df[cutoff:]\n",
    "    \n",
    "    if train['y'].isna().any() or test['y'].isna().any():\n",
    "        print(f\"NaNs detected in train or test data for {target}. Skipping.\")\n",
    "        return None, float('inf')\n",
    "    \n",
    "    try:\n",
    "        model = Prophet(growth='linear', seasonality_mode='multiplicative', changepoint_prior_scale=0.05, seasonality_prior_scale=10.0)\n",
    "        for regressor in regressors:\n",
    "            model.add_regressor(regressor)\n",
    "        model.fit(train)\n",
    "        \n",
    "        future = model.make_future_dataframe(periods=len(test), freq='M')\n",
    "        future = pd.merge(future, prophet_df[['ds'] + regressors], on='ds', how='left')\n",
    "        future = future.bfill().ffill() # Ensure regressors are filled for prediction\n",
    "        \n",
    "        forecast = model.predict(future)\n",
    "        forecast_test = forecast.iloc[len(train):]\n",
    "        \n",
    "        if forecast_test['yhat'].isna().any():\n",
    "            print(f\"NaNs in Prophet predictions for {target}. Skipping.\")\n",
    "            return None, float('inf')\n",
    "        \n",
    "        mse = mean_squared_error(test['y'], forecast_test['yhat'])\n",
    "        print(f\"Prophet MSE for {target}: {mse}\")\n",
    "        \n",
    "        with open('prophet_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        return forecast_test['yhat'], mse\n",
    "    except Exception as e:\n",
    "        print(f\"Prophet failed for {target}: {e}\")\n",
    "        return None, float('inf')\n",
    "\n",
    "# I'm using this function to go beyond simple forecasting.\n",
    "# By calculating correlations, I can understand relationships between energy sources.\n",
    "# The GARCH model helps me analyze and forecast volatility, a key measure of risk.\n",
    "# Finally, the Monte Carlo simulation provides a probabilistic view of future prices,\n",
    "# which is more robust than a single point forecast.\n",
    "def additional_analyses(df, target='Petrol'):\n",
    "    \"\"\"\n",
    "    Correlations, volatility (GARCH), and Monte Carlo simulations.\n",
    "    \"\"\"\n",
    "    print(f\"Additional analyses input columns: {list(df.columns)}\")\n",
    "    if target not in df.columns:\n",
    "        print(f\"Target column '{target}' not in DataFrame. Available columns: {list(df.columns)}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    corr = df.corr()\n",
    "    sns.heatmap(corr, annot=True, cmap='viridis')\n",
    "    plt.title('Energy Sources Correlations')\n",
    "    plt.savefig('correlations_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    returns = df[target].pct_change().dropna() * 100\n",
    "    if returns.empty:\n",
    "        print(f\"No valid returns data for {target}. Skipping GARCH.\")\n",
    "        return corr, None, None\n",
    "    try:\n",
    "        garch_model = arch_model(returns, vol='Garch', p=1, q=1)\n",
    "        garch_fit = garch_model.fit(disp='off')\n",
    "        vol_forecast = garch_fit.forecast(horizon=12)\n",
    "        print(\"12-Month Volatility Forecast:\", vol_forecast.variance.iloc[-1].values)\n",
    "    except Exception as e:\n",
    "        print(f\"GARCH model failed: {e}. Skipping.\")\n",
    "        vol_forecast = None\n",
    "    \n",
    "    last_price = df[target].iloc[-1]\n",
    "    vol = df[target].pct_change().std()\n",
    "    simulations = 100\n",
    "    periods = 12\n",
    "    paths = np.random.normal(0, vol, size=(periods, simulations))\n",
    "    future_prices = last_price * np.exp(np.cumsum(paths, axis=0))\n",
    "    mean_path = future_prices.mean(axis=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(mean_path)\n",
    "    plt.title('Monte Carlo Price Simulation')\n",
    "    plt.savefig('monte_carlo_sim.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return corr, vol_forecast, mean_path\n",
    "\n",
    "# This is the main orchestration function. It calls all the other functions\n",
    "# in a logical sequence, from data fetching to analysis.\n",
    "# I've used a comprehensive try-except block to handle any unhandled errors,\n",
    "# ensuring the pipeline fails gracefully and provides an informative message.\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrates the pipeline.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df, _ = fetch_energy_data()\n",
    "        transformed_df, original_df = preprocess_data(df)\n",
    "        df_with_sentiment = add_nlp_sentiment(original_df, sentiment_analyzer)\n",
    "        \n",
    "        target = 'Petrol' if 'Petrol' in df_with_sentiment.columns else df_with_sentiment.columns[0]\n",
    "        print(f\"Using target column: {target}\")\n",
    "        \n",
    "        arima_preds, arima_mse = train_arima(df_with_sentiment, target=target)\n",
    "        lstm_preds, lstm_mse = train_lstm(df_with_sentiment, target=target)\n",
    "        prophet_preds, prophet_mse = train_prophet(df_with_sentiment, target=target)\n",
    "        \n",
    "        corr, vol, sim = additional_analyses(df_with_sentiment, target=target)\n",
    "        \n",
    "        results = {\n",
    "            'arima_mse': arima_mse,\n",
    "            'lstm_mse': lstm_mse,\n",
    "            'prophet_mse': prophet_mse\n",
    "        }\n",
    "        pd.DataFrame([results]).to_csv('model_results.csv', index=False)\n",
    "        print(\"Pipeline finished successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unhandled error occurred in main.py: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Starting data processing pipeline...\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a588a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c869ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
