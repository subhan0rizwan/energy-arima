{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c869ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing pipeline...\n",
      "Successfully fetched Petrol: 2686 records\n",
      "Successfully fetched Natural_Gas: 2686 records\n",
      "Successfully fetched Crude_Oil: 2685 records\n",
      "Successfully fetched Uranium: 2685 records\n",
      "Final DataFrame shape: (129, 4)\n",
      "Preprocess input columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium']\n",
      "Preprocess output columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium', 'Petrol_rolling', 'Petrol_std', 'Natural_Gas_rolling', 'Natural_Gas_std', 'Crude_Oil_rolling', 'Crude_Oil_std', 'Uranium_rolling', 'Uranium_std']\n",
      "Petrol ADF p-value: 0.10428640301440367\n",
      "Natural_Gas ADF p-value: 0.07646257433152302\n",
      "Crude_Oil ADF p-value: 0.2582934051091767\n",
      "Uranium ADF p-value: 0.9957720528563957\n",
      "Transformed DataFrame columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium']\n",
      "Sentiment input columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium', 'Petrol_rolling', 'Petrol_std', 'Natural_Gas_rolling', 'Natural_Gas_std', 'Crude_Oil_rolling', 'Crude_Oil_std', 'Uranium_rolling', 'Uranium_std']\n",
      "Sentiment output columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium', 'Petrol_rolling', 'Petrol_std', 'Natural_Gas_rolling', 'Natural_Gas_std', 'Crude_Oil_rolling', 'Crude_Oil_std', 'Uranium_rolling', 'Uranium_std', 'Sentiment']\n",
      "Using target column: Petrol\n",
      "ARIMA input columns: ['Petrol', 'Natural_Gas', 'Crude_Oil', 'Uranium', 'Petrol_rolling', 'Petrol_std', 'Natural_Gas_rolling', 'Natural_Gas_std', 'Crude_Oil_rolling', 'Crude_Oil_std', 'Uranium_rolling', 'Uranium_std', 'Sentiment']\n",
      "ARIMA failed for Petrol: name 'auto_arima' is not defined\n",
      "An unhandled error occurred in main.py: name 'create_sequences' is not defined\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from arch import arch_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import pipeline\n",
    "from prophet import Prophet\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def fetch_energy_data(fallback=False):\n",
    "    \"\"\"\n",
    "    Fetches daily energy prices using YFinance for futures.\n",
    "    Returns a tuple (DataFrame, bool) to indicate success/fallback.\n",
    "    \"\"\"\n",
    "    if fallback:\n",
    "        print(\"Using synthetic data as fallback\")\n",
    "        dates = pd.date_range('2015-01-01', '2025-09-08', freq='D')\n",
    "        np.random.seed(42)\n",
    "        df = pd.DataFrame({\n",
    "            'Petrol': np.clip(np.random.normal(loc=100, scale=10, size=len(dates)), 50, 150),\n",
    "            'Uranium': np.clip(np.random.normal(loc=50, scale=5, size=len(dates)), 20, 80),\n",
    "            'Natural_Gas': np.clip(np.random.normal(loc=5, scale=0.5, size=len(dates)), 2, 8),\n",
    "            'Crude_Oil': np.clip(np.random.normal(loc=70, scale=7, size=len(dates)), 30, 110)\n",
    "        }, index=dates)\n",
    "        return df, True\n",
    "\n",
    "    tickers = {\n",
    "        'Petrol': 'RB=F',\n",
    "        'Natural_Gas': 'NG=F',\n",
    "        'Crude_Oil': 'CL=F',\n",
    "        'Uranium': 'URA'\n",
    "    }\n",
    "    dfs = []\n",
    "\n",
    "    # Get the earliest date available for a proxy ticker\n",
    "    try:\n",
    "        start_date = yf.download('CL=F', period='max').index.min()\n",
    "        print(f\"Starting data download from: {start_date.date()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not determine earliest date: {e}. Defaulting to 2015-01-01.\")\n",
    "        start_date = pd.to_datetime('2015-01-01')\n",
    "\n",
    "    # Download data for all tickers from the determined start date\n",
    "    for name, ticker in tickers.items():\n",
    "        try:\n",
    "            # Explicitly setting start date based on the longest available history\n",
    "            df = yf.download(ticker, start=start_date, progress=False, timeout=60)\n",
    "            if df.empty:\n",
    "                print(f\"No data returned for {name}.\")\n",
    "                continue\n",
    "            \n",
    "            df = df[['Close']].rename(columns={'Close': name})\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            dfs.append(df)\n",
    "            print(f\"Successfully fetched {name}: {len(df)} records.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {name}: {e}. Skipping this ticker.\")\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"No data fetched. Falling back to synthetic data.\")\n",
    "        return fetch_energy_data(fallback=True)\n",
    "\n",
    "    energy_df = pd.concat(dfs, axis=1, join='outer')\n",
    "    energy_df = energy_df.replace([np.inf, -np.inf], np.nan).dropna(how='all').ffill().bfill()\n",
    "    energy_df.index = pd.to_datetime(energy_df.index)\n",
    "\n",
    "    # Resample to monthly and fill NaNs\n",
    "    energy_df = energy_df.resample('M').mean().ffill().bfill()\n",
    "\n",
    "    # The rest of your function remains the same\n",
    "    # ...\n",
    "    energy_df.to_parquet('energy_prices.parquet')\n",
    "    print(f\"Final DataFrame shape: {energy_df.shape}\")\n",
    "    return energy_df, False\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Applies EDA and preprocessing to multi-source DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(how='all').ffill().bfill()\n",
    "    print(f\"Preprocess input columns: {list(df.columns)}\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[f'{col}_rolling'] = df[col].rolling(12).mean()\n",
    "        df[f'{col}_std'] = df[col].rolling(12).std()\n",
    "    \n",
    "    df = df.fillna(method='bfill')\n",
    "    df.columns = df.columns.astype(str)  # Ensure string column names\n",
    "    print(f\"Preprocess output columns: {list(df.columns)}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    for col in df.columns[:4]:\n",
    "        sns.lineplot(data=df, x=df.index, y=col, label=col)\n",
    "    plt.title('Energy Prices Over Time')\n",
    "    plt.savefig('energy_prices_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    transformed_dfs = {}\n",
    "    for col in df.columns[:4]:\n",
    "        if df[col].isna().all():\n",
    "            print(f\"Skipping {col}: All values are NaN\")\n",
    "            continue\n",
    "        col_data = df[col].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if len(col_data) < 2:\n",
    "            print(f\"Skipping {col}: Insufficient data after cleaning\")\n",
    "            continue\n",
    "        adf_result = adfuller(col_data)\n",
    "        print(f\"{col} ADF p-value: {adf_result[1]}\")\n",
    "        \n",
    "        if adf_result[1] > 0.05:\n",
    "            temp_df = pd.DataFrame({col: col_data})\n",
    "            temp_df['log'] = np.log(temp_df[col].replace(0, np.nan).fillna(method='ffill'))\n",
    "            temp_df['log_sqrt'] = np.sqrt(temp_df['log'].replace([np.inf, -np.inf], np.nan).fillna(method='bfill'))\n",
    "            temp_df['diff'] = temp_df['log_sqrt'].diff().dropna()\n",
    "            transformed_dfs[col] = temp_df['diff']\n",
    "        else:\n",
    "            transformed_dfs[col] = col_data\n",
    "    \n",
    "    transformed_df = pd.DataFrame(transformed_dfs)\n",
    "    transformed_df.columns = transformed_df.columns.astype(str)  # Ensure string column names\n",
    "    print(f\"Transformed DataFrame columns: {list(transformed_df.columns)}\")\n",
    "    return transformed_df, df\n",
    "\n",
    "def add_nlp_sentiment(df):\n",
    "    \"\"\"\n",
    "    Adds sentiment feature from simulated energy news/tweets.\n",
    "    \"\"\"\n",
    "    print(f\"Sentiment input columns: {list(df.columns)}\")\n",
    "try:\n",
    "    sentiment_analyzer = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english', framework='pt', device=-1)\n",
    "except Exception as e:\n",
    "    sentiment_analyzer = None\n",
    "    print(f\"Failed to load sentiment model: {e}\")\n",
    "\n",
    "def fetch_energy_data(fallback=False):\n",
    "    \"\"\"\n",
    "    Fetches daily energy prices using YFinance for futures (Petrol, Natural Gas, Crude Oil, Uranium).\n",
    "    Returns a tuple (DataFrame, bool) to indicate success/fallback.\n",
    "    \"\"\"\n",
    "    if fallback:\n",
    "        print(\"Using synthetic data as fallback\")\n",
    "        dates = pd.date_range('2015-01-01', '2025-09-08', freq='D')\n",
    "        np.random.seed(42)\n",
    "        df = pd.DataFrame({\n",
    "            'Petrol': np.clip(np.random.normal(loc=100, scale=10, size=len(dates)), 50, 150),\n",
    "            'Uranium': np.clip(np.random.normal(loc=50, scale=5, size=len(dates)), 20, 80),\n",
    "            'Natural_Gas': np.clip(np.random.normal(loc=5, scale=0.5, size=len(dates)), 2, 8),\n",
    "            'Crude_Oil': np.clip(np.random.normal(loc=70, scale=7, size=len(dates)), 30, 110)\n",
    "        }, index=dates)\n",
    "        print(f\"Synthetic DataFrame columns: {list(df.columns)}\")\n",
    "        return df, True\n",
    "\n",
    "    tickers = {\n",
    "        'Petrol': 'RB=F',\n",
    "        'Natural_Gas': 'NG=F',\n",
    "        'Crude_Oil': 'CL=F',\n",
    "        'Uranium': 'URA'\n",
    "    }\n",
    "    dfs = []\n",
    "    for name, ticker in tickers.items():\n",
    "        try:\n",
    "            df = yf.download(ticker, start='2015-01-01', end='2025-09-08', progress=False, timeout=60)\n",
    "            if df.empty:\n",
    "                print(f\"No data returned for {name}. Using synthetic data.\")\n",
    "                dates = pd.date_range('2015-01-01', '2025-09-08', freq='D')\n",
    "                mean, std = {'Petrol': (100, 10), 'Natural_Gas': (5, 0.5), 'Crude_Oil': (70, 7), 'Uranium': (50, 5)}[name]\n",
    "                synthetic_df = pd.DataFrame({name: np.random.normal(loc=mean, scale=std, size=len(dates))}, index=dates)\n",
    "                dfs.append(synthetic_df)\n",
    "                continue\n",
    "            df = df[['Close']].rename(columns={'Close': name})\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            dfs.append(df)\n",
    "            print(f\"Successfully fetched {name}: {len(df)} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {name}: {e}. Using synthetic data for {name}.\")\n",
    "            dates = pd.date_date_range('2015-01-01', '2025-09-08', freq='D')\n",
    "            mean, std = {'Petrol': (100, 10), 'Natural_Gas': (5, 0.5), 'Crude_Oil': (70, 7), 'Uranium': (50, 5)}[name]\n",
    "            synthetic_df = pd.DataFrame({name: np.random.normal(loc=mean, scale=std, size=len(dates))}, index=dates)\n",
    "            dfs.append(synthetic_df)\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"No data fetched. Falling back to synthetic data.\")\n",
    "        return fetch_energy_data(fallback=True)\n",
    "\n",
    "    energy_df = pd.concat(dfs, axis=1, join='outer')\n",
    "    energy_df.columns = [c[0] if isinstance(c, tuple) else c for c in energy_df.columns]\n",
    "    energy_df = energy_df.replace([np.inf, -np.inf], np.nan).dropna(how='all').ffill().bfill()\n",
    "    energy_df.index = pd.to_datetime(energy_df.index)\n",
    "    \n",
    "    if not energy_df.index.is_unique:\n",
    "        energy_df = energy_df[~energy_df.index.duplicated(keep='first')]\n",
    "\n",
    "    if not isinstance(energy_df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"Index is not a DatetimeIndex after concatenation\")\n",
    "\n",
    "    energy_df = energy_df.resample('M').mean().ffill().bfill()\n",
    "    for col in energy_df.columns:\n",
    "        mean, std = energy_df[col].mean(), energy_df[col].std()\n",
    "        energy_df[col] = energy_df[col].clip(lower=mean - 3*std, upper=mean + 3*std)\n",
    "\n",
    "    if energy_df.isna().any().any():\n",
    "        energy_df = energy_df.bfill()\n",
    "\n",
    "    energy_df.to_parquet('energy_prices.parquet')\n",
    "    print(f\"Final DataFrame shape: {energy_df.shape}\")\n",
    "    return energy_df, False\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Applies EDA and preprocessing to multi-source DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(how='all').ffill().bfill()\n",
    "    print(f\"Preprocess input columns: {list(df.columns)}\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[f'{col}_rolling'] = df[col].rolling(12).mean()\n",
    "        df[f'{col}_std'] = df[col].rolling(12).std()\n",
    "    \n",
    "    df = df.fillna(method='bfill')\n",
    "    df.columns = df.columns.astype(str)\n",
    "    print(f\"Preprocess output columns: {list(df.columns)}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    for col in df.columns[:4]:\n",
    "        sns.lineplot(data=df, x=df.index, y=col, label=col)\n",
    "    plt.title('Energy Prices Over Time')\n",
    "    plt.savefig('energy_prices_plot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    transformed_dfs = {}\n",
    "    for col in df.columns[:4]:\n",
    "        if df[col].isna().all():\n",
    "            print(f\"Skipping {col}: All values are NaN\")\n",
    "            continue\n",
    "        col_data = df[col].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if len(col_data) < 2:\n",
    "            print(f\"Skipping {col}: Insufficient data after cleaning\")\n",
    "            continue\n",
    "        adf_result = adfuller(col_data)\n",
    "        print(f\"{col} ADF p-value: {adf_result[1]}\")\n",
    "        \n",
    "        if adf_result[1] > 0.05:\n",
    "            temp_df = pd.DataFrame({col: col_data})\n",
    "            temp_df['log'] = np.log(temp_df[col].replace(0, np.nan).fillna(method='ffill'))\n",
    "            temp_df['log_sqrt'] = np.sqrt(temp_df['log'].replace([np.inf, -np.inf], np.nan).fillna(method='bfill'))\n",
    "            temp_df['diff'] = temp_df['log_sqrt'].diff().dropna()\n",
    "            transformed_dfs[col] = temp_df['diff']\n",
    "        else:\n",
    "            transformed_dfs[col] = col_data\n",
    "    \n",
    "    transformed_df = pd.DataFrame(transformed_dfs)\n",
    "    transformed_df.columns = transformed_df.columns.astype(str)\n",
    "    print(f\"Transformed DataFrame columns: {list(transformed_df.columns)}\")\n",
    "    return transformed_df, df\n",
    "\n",
    "def add_nlp_sentiment(df, analyzer):\n",
    "    \"\"\"\n",
    "    Adds sentiment feature from simulated energy news/tweets.\n",
    "    \"\"\"\n",
    "    print(f\"Sentiment input columns: {list(df.columns)}\")\n",
    "    if analyzer is None:\n",
    "        print(\"Sentiment analysis model not loaded. Adding zero sentiment.\")\n",
    "        df['Sentiment'] = 0.0\n",
    "        df.columns = df.columns.astype(str)\n",
    "        return df\n",
    "    \n",
    "    try:\n",
    "        sample_texts = [\n",
    "            \"OPEC cuts production, oil prices surge\",\n",
    "            \"Natural gas reserves high, prices drop\",\n",
    "            \"Renewable energy boom affects crude demand\",\n",
    "            \"Geopolitical tensions in Middle East spike petrol prices\"\n",
    "        ] * (len(df) // 4 + 1)\n",
    "        \n",
    "        scores = []\n",
    "        for text in sample_texts[:len(df)]:\n",
    "            result = analyzer(text)[0]\n",
    "            score = result['score'] if result['label'] == 'POSITIVE' else -result['score']\n",
    "            scores.append(score)\n",
    "        \n",
    "        df['Sentiment'] = pd.Series(scores, index=df.index[:len(scores)]).rolling(5).mean().ffill().bfill()\n",
    "    except Exception as e:\n",
    "        print(f\"Sentiment analysis failed: {e}. Adding zero sentiment.\")\n",
    "        df['Sentiment'] = 0.0\n",
    "    df.columns = df.columns.astype(str)\n",
    "    print(f\"Sentiment output columns: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def train_arima(df, target='Petrol'):\n",
    "    \"\"\"\n",
    "    ARIMA model with auto_arima for optimal parameter selection and exogenous regressors.\n",
    "    \"\"\"\n",
    "    print(f\"ARIMA input columns: {list(df.columns)}\")\n",
    "    if target not in df.columns:\n",
    "        print(f\"Target column '{target}' not in DataFrame. Available columns: {list(df.columns)}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Exogenous variables (all other columns)\n",
    "    features = [col for col in df.columns if col != target and 'rolling' in col and 'std' in col or 'Sentiment' in col]\n",
    "    X = df[features].ffill().bfill()\n",
    "    \n",
    "    cutoff = int(len(df) * 0.65)\n",
    "    train, test = df[target][:cutoff], df[target][cutoff:]\n",
    "    X_train, X_test = X[:cutoff], X[cutoff:]\n",
    "\n",
    "    if train.isna().any() or test.isna().any():\n",
    "        print(f\"NaNs detected in train or test data for {target}. Skipping.\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Use auto_arima to find the best p, d, q parameters\n",
    "        # and include other columns as exogenous variables\n",
    "        model_fit = auto_arima(\n",
    "            train, \n",
    "            exogenous=X_train, \n",
    "            seasonal=True, # Enable seasonal component\n",
    "            m=12,          # Monthly data, so seasonality is 12\n",
    "            stepwise=True,\n",
    "            suppress_warnings=True,\n",
    "            error_action='ignore',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        preds = model_fit.predict(n_periods=len(test), exogenous=X_test)\n",
    "        \n",
    "        if np.any(np.isnan(preds)):\n",
    "            print(f\"NaNs in ARIMA predictions for {target}. Skipping.\")\n",
    "            return None, None\n",
    "        \n",
    "        mse = mean_squared_error(test, preds)\n",
    "        print(f\"ARIMAX MSE for {target}: {mse}\")\n",
    "        \n",
    "        with open('arima_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model_fit, f)\n",
    "        return preds, mse\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA failed for {target}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def train_lstm(df, target='Petrol', seq_length=12):\n",
    "    \"\"\"\n",
    "    LSTM model with improved architecture and regularization.\n",
    "    \"\"\"\n",
    "    # ... (existing data preprocessing and scaling code)\n",
    "    # ...\n",
    "    X_train, y_train = create_sequences(scaled_df.values[:cutoff], seq_length)\n",
    "    X_test, y_test = create_sequences(scaled_df.values[cutoff:], seq_length)\n",
    "    \n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(f\"Insufficient valid sequences for LSTM. Returning None.\")\n",
    "        return None, None\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    \n",
    "    class LSTM(nn.Module):\n",
    "        def __init__(self, input_size=len(df.columns), hidden_size=50, num_layers=2, dropout_rate=0.2):\n",
    "            super().__init__()\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            out, _ = self.lstm(x)\n",
    "            return self.fc(out[:, -1, :])\n",
    "    \n",
    "    # Create the model with Dropout and more epochs\n",
    "    model = LSTM(input_size=len(scaled_df.columns), hidden_size=100, num_layers=3, dropout_rate=0.3)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(200): # Increase epochs for better training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_test).squeeze().numpy()\n",
    "    \n",
    "    full_scaler = MinMaxScaler().fit(df[target].values.reshape(-1,1))\n",
    "    \n",
    "    # Adjust y_test for correct inverse transformation\n",
    "    y_test_full_scaled = full_scaler.transform(y_test.reshape(-1, 1))\n",
    "    preds_full_scaled = full_scaler.transform(preds.reshape(-1, 1))\n",
    "    \n",
    "    mse = mean_squared_error(y_test_full_scaled, preds_full_scaled)\n",
    "    print(f\"LSTM MSE for {target}: {mse}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), 'lstm_model.pth')\n",
    "    return preds, mse\n",
    "\n",
    "\n",
    "def train_prophet(df, target='Petrol'):\n",
    "    \"\"\"\n",
    "    Prophet model with hyperparameter tuning and external regressors.\n",
    "    \"\"\"\n",
    "    print(f\"Prophet input columns: {list(df.columns)}\")\n",
    "    if target not in df.columns:\n",
    "        print(f\"Target column '{target}' not in DataFrame. Available columns: {list(df.columns)}\")\n",
    "        return None, None\n",
    "\n",
    "    prophet_df = df.reset_index().rename(columns={'index': 'ds', target: 'y'})\n",
    "    \n",
    "    # Add other energy sources and sentiment as regressors\n",
    "    regressors = [col for col in df.columns if col != target]\n",
    "    for regressor in regressors:\n",
    "        prophet_df[regressor] = df[regressor].ffill().bfill().values\n",
    "    \n",
    "    cutoff = int(len(prophet_df) * 0.65)\n",
    "    train, test = prophet_df[:cutoff], prophet_df[cutoff:]\n",
    "    \n",
    "    if train['y'].isna().any() or test['y'].isna().any():\n",
    "        print(f\"NaNs detected in train or test data for {target}. Skipping.\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Initialize Prophet with tuning parameters\n",
    "        model = Prophet(\n",
    "            growth='linear',\n",
    "            seasonality_mode='multiplicative', # Multiplicative seasonality for financial data\n",
    "            changepoint_prior_scale=0.05,\n",
    "            seasonality_prior_scale=10.0\n",
    "        )\n",
    "        \n",
    "        # Add the external regressors\n",
    "        for regressor in regressors:\n",
    "            model.add_regressor(regressor)\n",
    "\n",
    "        model.fit(train[['ds', 'y'] + regressors])\n",
    "        \n",
    "        future = model.make_future_dataframe(periods=len(test), freq=df.index.freq)\n",
    "        future = pd.merge(future, prophet_df[['ds'] + regressors], on='ds', how='left')\n",
    "        \n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        if forecast['yhat'].isna().any():\n",
    "            print(f\"NaNs in Prophet predictions for {target}. Skipping.\")\n",
    "            return None, None\n",
    "        \n",
    "        # Match forecast and test data for MSE calculation\n",
    "        forecast_test = forecast.iloc[cutoff:]\n",
    "        \n",
    "        mse = mean_squared_error(test['y'], forecast_test['yhat'])\n",
    "        print(f\"Prophet MSE for {target}: {mse}\")\n",
    "        \n",
    "        with open('prophet_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        return forecast_test['yhat'], mse\n",
    "    except Exception as e:\n",
    "        print(f\"Prophet failed for {target}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def additional_analyses(df, target='Petrol'):\n",
    "    \"\"\"\n",
    "    Correlations, volatility (GARCH), and Monte Carlo simulations.\n",
    "    \"\"\"\n",
    "    print(f\"Additional analyses input columns: {list(df.columns)}\")\n",
    "    if target not in df.columns:\n",
    "        print(f\"Target column '{target}' not in DataFrame. Available columns: {list(df.columns)}\")\n",
    "        return None, None, None\n",
    "    corr = df.corr()\n",
    "    sns.heatmap(corr, annot=True)\n",
    "    plt.title('Energy Sources Correlations')\n",
    "    plt.savefig('correlations_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    returns = df[target].pct_change().dropna() * 100\n",
    "    if returns.empty:\n",
    "        print(f\"No valid returns data for {target}. Skipping GARCH.\")\n",
    "        return corr, None, None\n",
    "    garch_model = arch_model(returns, vol='Garch', p=1, q=1)\n",
    "    garch_fit = garch_model.fit(disp='off')\n",
    "    vol_forecast = garch_fit.forecast(horizon=12)\n",
    "    print(\"12-Month Volatility Forecast:\", vol_forecast.variance.iloc[-1])\n",
    "    \n",
    "    last_price = df[target].iloc[-1]\n",
    "    vol = df[target].pct_change().std()\n",
    "    simulations = 100\n",
    "    periods = 12\n",
    "    paths = np.random.normal(0, vol, size=(periods, simulations))\n",
    "    future_prices = last_price * np.exp(np.cumsum(paths, axis=0))\n",
    "    mean_path = future_prices.mean(axis=1)\n",
    "    \n",
    "    plt.plot(mean_path)\n",
    "    plt.title('Monte Carlo Price Simulation')\n",
    "    plt.savefig('monte_carlo_sim.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return corr, vol_forecast, mean_path\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrates the pipeline.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Correctly unpack the tuple returned by fetch_energy_data\n",
    "        df, _ = fetch_energy_data(fallback=False)\n",
    "        transformed_df, original_df = preprocess_data(df)\n",
    "        df_with_sentiment = add_nlp_sentiment(original_df, sentiment_analyzer)\n",
    "        \n",
    "        target = 'Petrol' if 'Petrol' in df_with_sentiment.columns else df_with_sentiment.columns[0]\n",
    "        print(f\"Using target column: {target}\")\n",
    "        \n",
    "        arima_preds, arima_mse = train_arima(df_with_sentiment, target=target)\n",
    "        lstm_preds, lstm_mse = train_lstm(df_with_sentiment, target=target)\n",
    "        \n",
    "        if lstm_preds is None:\n",
    "            print(\"LSTM failed. Skipping LSTM results.\")\n",
    "            lstm_mse = float('inf')\n",
    "        \n",
    "        prophet_preds, prophet_mse = train_prophet(df_with_sentiment, target=target)\n",
    "        \n",
    "        corr, vol, sim = additional_analyses(df_with_sentiment, target=target)\n",
    "        \n",
    "        results = {\n",
    "            'arima_mse': arima_mse if arima_mse is not None else float('inf'),\n",
    "            'lstm_mse': lstm_mse if lstm_mse is not None else float('inf'),\n",
    "            'prophet_mse': prophet_mse if prophet_mse is not None else float('inf')\n",
    "        }\n",
    "        pd.DataFrame([results]).to_csv('model_results.csv')\n",
    "    except Exception as e:\n",
    "        print(f\"An unhandled error occurred in main.py: {e}\")\n",
    "        # Return a non-zero exit code to signal failure\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Starting data processing pipeline...\")\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
